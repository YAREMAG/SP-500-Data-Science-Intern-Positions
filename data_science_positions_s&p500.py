# -*- coding: utf-8 -*-
"""Data Science Positions - S&P500.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18wDrM66tZ2hshHOHQTo1jJHk4MjIbqsU
"""

import requests
from bs4 import BeautifulSoup

def get_sp500_companies():
    # URL of the Wikipedia page listing S&P 500 companies
    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'

    # Send an HTTP request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the table containing the list of S&P 500 companies
        table = soup.find('table', {'class': 'wikitable sortable'})

        # Initialize an empty list to store company names
        company_names = []

        # Iterate through rows of the table (excluding the header row)
        for row in table.find_all('tr')[1:]:
            # Get the first column (which contains the company names)
            company_name = row.find_all('td')[1].text.strip()
            company_names.append(company_name)

        return company_names
    else:
        # Print an error message if the request was not successful
        print('Failed to retrieve data from Wikipedia.')
        return None

# Example usage
if __name__ == "__main__":
    sp500_companies = get_sp500_companies()

import requests
from bs4 import BeautifulSoup
import re

def get_sp500_company_websites():
    # URL of the Wikipedia page listing S&P 500 companies
    url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'

    # Send an HTTP request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the table containing the list of S&P 500 companies
        table = soup.find('table', {'class': 'wikitable sortable'})

        # Initialize an empty dictionary to store company names and websites
        company_websites = {}

        # Iterate through rows of the table (excluding the header row)
        for row in table.find_all('tr')[1:]:
            # Get the first column (which contains the company names)
            company_cell = row.find_all('td')[1]
            # Extract the company name and its Wikipedia link
            company_name = company_cell.text.strip()
            company_link = company_cell.find('a')['href'] if company_cell.find('a') else None

            # If the Wikipedia link exists, fetch the content and find the website link
            if company_link:
                company_website = extract_company_website(company_link)
                company_websites[company_name] = company_website

        return company_websites
    else:
        # Print an error message if the request was not successful
        print('Failed to retrieve data from Wikipedia.')
        return None

def extract_company_website(wiki_link):
    # Construct the full URL of the Wikipedia page for the company
    full_url = 'https://en.wikipedia.org' + wiki_link

    # Send an HTTP request to the company's Wikipedia page
    response = requests.get(full_url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.content, 'html.parser')

        # Find the infobox table containing the company details
        infobox_table = soup.find('table', {'class': 'infobox vcard'})

        # Search for the website link within the infobox
        if infobox_table:
            for row in infobox_table.find_all('tr'):
                th = row.find('th')
                if th and th.text.strip().lower() == 'website':
                    td = row.find('td')
                    if td:
                        website_link = td.find('a')
                        if website_link:
                            return website_link['href']
        else:
            # Search for website link in the page content if not found in the infobox
            page_content = soup.find('div', {'class': 'mw-parser-output'})
            if page_content:
                website_regex = re.compile(r'https?://[^\s]+')
                website_links = website_regex.findall(str(page_content))
                if website_links:
                    return website_links[0]  # Return the first website link found

    return None


# Example usage
if __name__ == "__main__":
    sp500_company_websites = get_sp500_company_websites()
    if sp500_company_websites:
        for company, website in sp500_company_websites.items():
            print(f"Company: {company}, Website: {website}")

print(sp500_company_websites)

website_links = list(sp500_company_websites.values())
print(website_links[5])

print(sp500_companies[0])

pip install openpyxl

from openpyxl import Workbook
from googlesearch import search

# Create a new Excel workbook
wb = Workbook()
ws = wb.active

# Perform Google search
query = '(site:lever.co OR site:greenhouse.io) ("Data Science Intern" OR "Junior Data Scientist") ("internship" OR "1 year of experience")'
results = search(query, num_results=100)

# Write the results to Excel
row = 1
for url in results:
    ws.cell(row, 1, url)
    row += 1
    time.sleep(2)

# Save the workbook
wb.save("search_results.xlsx")

print("Search results have been added to the Excel file.")